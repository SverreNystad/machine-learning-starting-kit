{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_working_directory = os.getcwd()\n",
    "\n",
    "# Go up one level from the current working directory\n",
    "parent_directory = os.path.join(current_working_directory, '..')\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(parent_directory)\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "from src.data.data_loader import LocalDataLoader, MockDataLoader\n",
    "x, y = LocalDataLoader().load_raw_data()\n",
    "training_data = pd.concat([x, y], axis=1)\n",
    "# Display basic information\n",
    "print(training_data.info())\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nan analysis\n",
    "What fields contains NaN values? How large part are they? How to handle them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find amount of nans in each column\n",
    "# amount / total \n",
    "nans = pd.DataFrame()\n",
    "nans[\"amount_nans\"] = training_data.isnull().sum()\n",
    "nans[\"%\"] = (nans[\"amount_nans\"] / training_data.shape[0]) \n",
    "nans = nans[nans[\"amount_nans\"] > 0] \n",
    "print(nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies to deal with NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates Analysis\n",
    "Are there any duplicates in the dataset and in that case how many are there of them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_count = training_data.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of dimensionality\n",
    "Numerous machine learning challenges encompass training instances that are characterized by thousands, if not millions, of features. The sheer volume of features not only considerably slows down the training process but also complicates the task of identifying effective solutions, this is known as the **curse of dimensionality**.\n",
    "\n",
    "Dimensionality reduction is a technique to reduce the number of features in the dataset. \n",
    "Dimensionality reduction, while beneficial in speeding up training processes, inherently involves a compromise similar to image compression (e.g., converting an image to JPEG format), where some level of information loss occurs. This trade-off can potentially affect the performance of your model, making it slightly less accurate. Additionally, implementing dimensionality reduction adds complexity to your data processing pipelines.\n",
    "\n",
    "It's advisable to first attempt training your model with the complete dataset. Dimensionality reduction should be considered a secondary option if the training process proves to be excessively time-consuming. In certain scenarios, reducing the dataset's dimensions may actually improve model performance by eliminating noise and irrelevant details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "percentage_variance = 0.95\n",
    "pca = PCA()\n",
    "pca.fit(x, y)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "dimensions = np.argmax(cumsum >= percentage_variance) + 1\n",
    "print(f\"Number of dimensions to keep {percentage_variance}% of the variance: {dimensions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: *If the dataset is to large to fit into memory run the IncrementalPCA algorithm. Sklearn has a built-in class for this purpose.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_amount_of_dimensions = x.shape[1]\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "plt.axis([0, total_amount_of_dimensions, 0, 1])\n",
    "plt.plot([dimensions, dimensions], [0, percentage_variance], \"k:\")\n",
    "plt.plot([0, dimensions], [percentage_variance, percentage_variance], \"k:\")\n",
    "plt.plot(dimensions, percentage_variance, \"ko\")\n",
    "plt.title(\"Explained variance as a function of the number of dimensions\")\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold Learning\n",
    "Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reduced_data(X_reduced: pd.DataFrame, title: str, y: pd.DataFrame=y):\n",
    "    \n",
    "    plt.figure(figsize=(11,4))\n",
    "    plt.title(title, fontsize=15)\n",
    "    print(X_reduced.shape)\n",
    "    if X_reduced.shape[1] == 1:\n",
    "        plt.plot(X_reduced, np.zeros(X_reduced.shape), \"b.\")\n",
    "    if X_reduced.shape[1] == 2:\n",
    "        plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y)\n",
    "    if X_reduced.shape[1] > 2:\n",
    "        plt.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y)\n",
    "    plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "    plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally Linear Embedding (LLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "X_reduced = lle.fit_transform(x)\n",
    "plot_reduced_data(X_reduced, \"Locally Linear Embedding (LLE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional Scaling (MDS)\n",
    "\n",
    "NB: This might be slow to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "mds = MDS(n_components=2, random_state=42)\n",
    "X_reduced_mds = mds.fit_transform(x)\n",
    "plot_reduced_data(X_reduced_mds, \"Multi-Dimensional Scaling (MDS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IsoMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "isomap = Isomap(n_components=2)\n",
    "X_reduced_isomap = isomap.fit_transform(x)\n",
    "plot_reduced_data(X_reduced_isomap, \"Isomap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_reduced_tsne = tsne.fit_transform(x)\n",
    "plot_reduced_data(X_reduced_tsne, \"t-SNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "lda.fit(x, y)\n",
    "X_reduced_lda = lda.transform(x)\n",
    "plot_reduced_data(X_reduced_lda, \"LDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
